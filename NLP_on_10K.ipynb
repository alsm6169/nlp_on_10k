{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "Goal here is to apply NLP to perform analysis on the published financial reports, more specifically on the annual 10-K filings(and later 10-Q i.e quaterly filings) for the listed companies.\n",
    "\n",
    "## What are 10-K filings?\n",
    "A 10-K is a comprehensive report filed annually by a publicly-traded company about its financial performance and is required by the U.S. Securities and Exchange Commission (SEC).  \n",
    "Source: [Investopedia](https://www.investopedia.com/terms/1/10-k.asp) \n",
    "\n",
    "## Where can one get 10-K filings in a machine readable format?\n",
    "10-K filings are available on sec website and can be easily searched via their tool [EDGAR](https://www.sec.gov/edgar/searchedgar/companysearch.html). EDGAR is the Electronic Data Gathering, Analysis, and Retrieval system used at the U.S. Securities and Exchange Commission (SEC). These are can be also accessed via their apis as will be done below.\n",
    "\n",
    "## Which sections of 10-K filings will be used in analyis?\n",
    "The details of different sections of 10-K filings can be found [here](https://www.sec.gov/fast-answers/answersreada10khtm.html).\n",
    "The sections that will be used for analysis are:\n",
    "- Item 1A - “Risk Factors” \n",
    "- Item 3 - “Legal Proceedings”\n",
    "- Item 7 - “Management’s Discussion and Analysis of Financial Condition and Results of Operations”\n",
    "\n",
    "## Exactly what NLP analysis will be done here?\n",
    "- Topic Modelling\n",
    "- Higlighting Trends\n",
    "- TBC\n",
    "\n",
    "## Which libraries are going to be used for NLP analysis\n",
    "- [Spacy](https://spacy.io/)\n",
    "- [Gensim](https://radimrehurek.com/gensim/)\n",
    "- BeautifulSoup and other python libraries\n",
    "\n",
    "## Inspiration\n",
    "I learnt about EDGAR during my Udacity course `AI for Trading`. I highly recommend this course for the large practical knowldge the course provided. In the course sentiment analysis was performed on 10-K statements and then further used to create factors into a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Packages\n",
    "[installing-python-packages-from-jupyter](https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==1.0.5 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.0.5)\n",
      "Requirement already satisfied: numpy==1.18.5 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from -r requirements.txt (line 3)) (1.18.5)\n",
      "Requirement already satisfied: requests==2.24.0 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from -r requirements.txt (line 4)) (2.24.0)\n",
      "Requirement already satisfied: scikit-learn==0.23.1 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from -r requirements.txt (line 5)) (0.23.1)\n",
      "Requirement already satisfied: tqdm==4.47.0 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from -r requirements.txt (line 6)) (4.47.0)\n",
      "Requirement already satisfied: beautifulsoup4==4.9.1 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (4.9.1)\n",
      "Requirement already satisfied: spacy==2.3.1 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from -r requirements.txt (line 8)) (2.3.1)\n",
      "Requirement already satisfied: gensim==3.8.0 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from -r requirements.txt (line 9)) (3.8.0)\n",
      "Requirement already satisfied: ratelimit==2.2.0 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from -r requirements.txt (line 10)) (2.2.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from pandas==1.0.5->-r requirements.txt (line 2)) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from pandas==1.0.5->-r requirements.txt (line 2)) (2.8.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from requests==2.24.0->-r requirements.txt (line 4)) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from requests==2.24.0->-r requirements.txt (line 4)) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from requests==2.24.0->-r requirements.txt (line 4)) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from requests==2.24.0->-r requirements.txt (line 4)) (1.25.9)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from scikit-learn==0.23.1->-r requirements.txt (line 5)) (0.16.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from scikit-learn==0.23.1->-r requirements.txt (line 5)) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from scikit-learn==0.23.1->-r requirements.txt (line 5)) (2.1.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from beautifulsoup4==4.9.1->-r requirements.txt (line 7)) (2.0.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from spacy==2.3.1->-r requirements.txt (line 8)) (1.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from spacy==2.3.1->-r requirements.txt (line 8)) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from spacy==2.3.1->-r requirements.txt (line 8)) (47.3.1.post20200622)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from spacy==2.3.1->-r requirements.txt (line 8)) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from spacy==2.3.1->-r requirements.txt (line 8)) (3.0.2)\n",
      "Requirement already satisfied: thinc==7.4.1 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from spacy==2.3.1->-r requirements.txt (line 8)) (7.4.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from spacy==2.3.1->-r requirements.txt (line 8)) (0.9.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from spacy==2.3.1->-r requirements.txt (line 8)) (0.6.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from spacy==2.3.1->-r requirements.txt (line 8)) (2.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from spacy==2.3.1->-r requirements.txt (line 8)) (1.0.2)\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from gensim==3.8.0->-r requirements.txt (line 9)) (1.15.0)\n",
      "Requirement already satisfied: smart-open>=1.7.0 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from gensim==3.8.0->-r requirements.txt (line 9)) (2.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.3.1->-r requirements.txt (line 8)) (1.7.0)\n",
      "Requirement already satisfied: boto in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim==3.8.0->-r requirements.txt (line 9)) (2.49.0)\n",
      "Requirement already satisfied: boto3 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from smart-open>=1.7.0->gensim==3.8.0->-r requirements.txt (line 9)) (1.9.66)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy==2.3.1->-r requirements.txt (line 8)) (3.1.0)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.66 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from boto3->smart-open>=1.7.0->gensim==3.8.0->-r requirements.txt (line 9)) (1.12.189)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from boto3->smart-open>=1.7.0->gensim==3.8.0->-r requirements.txt (line 9)) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from boto3->smart-open>=1.7.0->gensim==3.8.0->-r requirements.txt (line 9)) (0.1.13)\n",
      "Requirement already satisfied: docutils>=0.10 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from botocore<1.13.0,>=1.12.66->boto3->smart-open>=1.7.0->gensim==3.8.0->-r requirements.txt (line 9)) (0.16)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (2.3.1)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.9.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.18.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.6.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.47.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: setuptools in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (47.3.1.post20200622)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: thinc==7.4.1 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.7.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages/en_core_web_sm\n",
      "-->\n",
      "/Users/anirudh/opt/anaconda3/envs/nlp10k/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "# download the spacy model\n",
    "\n",
    "## enable this for downloading & using smaller spacy model\n",
    "!{sys.executable} -m spacy download en \n",
    "spacy_model = 'en'\n",
    "\n",
    "## enable this for downloading & using medium spacy model\n",
    "# !{sys.executable} -m spacy download en_core_web_md\n",
    "# spacy_model = 'en_core_web_md'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(spacy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "# ratelimit prevents large number of requests being sent to the server\n",
    "# @sleep_and_retry sleeps and then retries when number of requests are many\n",
    "# @limits defines the limits\n",
    "\n",
    "class SecAPI(object):\n",
    "    SEC_CALL_LIMIT = {'calls': 10, 'seconds': 1}\n",
    "\n",
    "    @staticmethod\n",
    "    @sleep_and_retry\n",
    "    # Dividing the call limit by half to avoid coming close to the limit\n",
    "    @limits(calls=SEC_CALL_LIMIT['calls'] / 2, period=SEC_CALL_LIMIT['seconds'])\n",
    "    def _call_sec(url):\n",
    "        return requests.get(url)\n",
    "\n",
    "    def get(self, url):\n",
    "        return self._call_sec(url).text\n",
    "\n",
    "sec_api = SecAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need cik_map to download form EDGAR\n",
    "cik_map = {\n",
    "    'tech_cik' : {\n",
    "        'AMZN': '0001018724',\n",
    "        'AAPL': '0000320193',\n",
    "        'MSFT': '0000789019',\n",
    "        'GOOG' : '0001652044'\n",
    "    },\n",
    "    'fin_cik' : {\n",
    "        'UBS' : '0001610520',\n",
    "        'CS'  : '0001159510',\n",
    "        'GSBD': '0001572694',\n",
    "        'JPM' : '0000019617'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'UBS': '0001610520', 'CS': '0001159510', 'GSBD': '0001572694', 'JPM': '0000019617'}\n"
     ]
    }
   ],
   "source": [
    "cik_lookup = cik_map['fin_cik']\n",
    "print(cik_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "def get_sec_data(sec_api, ticker, cik, doc_type='10-K', max_num=3):\n",
    "    start=0\n",
    "    count=60\n",
    "    rss_url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany' \\\n",
    "        '&CIK={}&type={}&start={}&count={}&owner=exclude&output=atom' \\\n",
    "        .format(cik, doc_type, start, count)\n",
    "    print(f'rss_url: {rss_url}')\n",
    "    sec_data = sec_api.get(rss_url)\n",
    "    # print(type(sec_data))\n",
    "    # print(f'sec_data: {sec_data}')\n",
    "    feed = BeautifulSoup(sec_data.encode()).feed\n",
    "    # print('feed: ', type(feed))\n",
    "    # print(feed)\n",
    "    entries=[]\n",
    "    cnt=0\n",
    "    for entry in feed.find_all('entry', recursive=False):\n",
    "        if entry.content.find('filing-type').getText() == doc_type and cnt < max_num:\n",
    "            dt_str = entry.content.find('filing-date').getText()\n",
    "            entries.append((ticker, \n",
    "                            entry.content.find('filing-date').getText()\n",
    "                            ,entry.content.find('filing-href').getText()\n",
    "                           ))\n",
    "            # entries[datetime.strptime(dt_str,'%Y-%m-%d').date()] = entry.content.find('filing-href').getText()\n",
    "            cnt += 1\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rss_url: https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0001652044&type=10-K&start=0&count=60&owner=exclude&output=atom\n",
      "[('GOOG', '2020-02-04', 'https://www.sec.gov/Archives/edgar/data/1652044/000165204420000008/0001652044-20-000008-index.htm'), ('GOOG', '2019-02-05', 'https://www.sec.gov/Archives/edgar/data/1652044/000165204419000004/0001652044-19-000004-index.htm'), ('GOOG', '2018-02-06', 'https://www.sec.gov/Archives/edgar/data/1652044/000165204418000007/0001652044-18-000007-index.htm'), ('GOOG', '2017-02-03', 'https://www.sec.gov/Archives/edgar/data/1652044/000165204417000008/0001652044-17-000008-index.htm'), ('GOOG', '2016-02-11', 'https://www.sec.gov/Archives/edgar/data/1652044/000165204416000012/0001652044-16-000012-index.htm')]\n"
     ]
    }
   ],
   "source": [
    "ticker = 'GOOG'\n",
    "cik = cik_map['tech_cik'][ticker]\n",
    "# cik = cik_map['fin_cik']['UBS']\n",
    "\n",
    "sec_data = get_sec_data(sec_api, ticker, cik,max_num=5)\n",
    "# print(sec_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df = df.append(sec_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>filing_date</th>\n",
       "      <th>href</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>2020-02-04</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/165204...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>2019-02-05</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/165204...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>2018-02-06</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/165204...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>2017-02-03</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/165204...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>2016-02-11</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/165204...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker filing_date                                               href\n",
       "0   GOOG  2020-02-04  https://www.sec.gov/Archives/edgar/data/165204...\n",
       "1   GOOG  2019-02-05  https://www.sec.gov/Archives/edgar/data/165204...\n",
       "2   GOOG  2018-02-06  https://www.sec.gov/Archives/edgar/data/165204...\n",
       "3   GOOG  2017-02-03  https://www.sec.gov/Archives/edgar/data/165204...\n",
       "4   GOOG  2016-02-11  https://www.sec.gov/Archives/edgar/data/165204..."
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns=['ticker','filing_date','href']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_filings(df_row):\n",
    "    print(f\"Downloading {df_row['ticker']}, for date: {df_row['filing_date']}\")\n",
    "    file_url = df_row['href'].replace('-index.htm', '.txt').replace('.txtl', '.txt')            \n",
    "    return sec_api.get(file_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dowloading GOOG, for date: 2020-02-04\n",
      "Dowloading GOOG, for date: 2019-02-05\n",
      "Dowloading GOOG, for date: 2018-02-06\n",
      "Dowloading GOOG, for date: 2017-02-03\n",
      "Dowloading GOOG, for date: 2016-02-11\n"
     ]
    }
   ],
   "source": [
    "df['filing'] = df.apply(download_filings,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SEC-DOCUMENT>0001652044-20-000008.txt : 20200204\n",
      "<SEC-HEADER>0001652044-20-000008.hdr.sgml : 20200204\n",
      "<ACCEPTANCE-DATETIME>20200203210359\n",
      "ACCESSION NUMBER:\t\t0001652044-20-000008\n",
      "CONFORMED SUBMISSION TYPE:\t10-K\n",
      "PUBLIC DOCUMENT COUNT:\t\t112\n",
      "CONFORMED PERIOD OF REPORT:\t20191231\n",
      "FILED AS OF DATE:\t\t20200204\n",
      "DATE AS OF CHANGE:\t\t20200203\n",
      "\n",
      "FILER:\n",
      "\n",
      "\tCOMPANY DATA:\t\n",
      "\t\tCOMPANY CONFORMED NAME:\t\t\tAlphabet Inc.\n",
      "\t\tCENTRAL INDEX KEY:\t\t\t0001652044\n",
      "\t\tSTANDARD INDUSTRIAL CLASSIFICATION:\tSERVICES-COMPUTER PROGRAMMING, DATA PROCESSING, ETC. [7370]\n",
      "\t\tIRS NUMBER:\t\t\t\t611767919\n",
      "\t\tSTATE OF INCORPORATION:\t\t\tDE\n",
      "\t\tFISCAL YEAR END:\t\t\t1231\n",
      "\n",
      "\tFILING VALUES:\n",
      "\t\tFORM TYPE:\t\t10-K\n",
      "\t\tSEC ACT:\t\t1934 Act\n",
      "\t\tSEC FILE NUMBER:\t001-37580\n",
      "\t\tFILM NUMBER:\t\t20570407\n",
      "\n",
      "\tBUSINESS ADDRESS:\t\n",
      "\t\tSTREET 1:\t\t1600 AMPHITHEATRE PARKWAY\n",
      "\t\tCITY:\t\t\tMOUNTAIN VIEW\n",
      "\t\tSTATE:\t\t\tCA\n",
      "\t\tZIP:\t\t\t94043\n",
      "\t\tBUSINESS PHONE:\t\t650-253-0000\n",
      "\n",
      "\tMAIL ADDRESS:\t\n",
      "\t\tSTREET 1:\t\t1600 AMPHITHEATRE PARKWAY\n",
      "\t\tCITY:\t\t\tMOUNTAIN VIEW\n",
      "\t\tSTATE:\t\t\tCA\n",
      "\t\tZIP:\t\t\t94043\n",
      "</SEC-HEADER>\n",
      "<DOCUMENT>\n",
      "<TYPE>10-K\n",
      "<SEQUENCE>1\n",
      "<FILENAME>goog10-k2019.htm\n",
      "<DESCRIPTION>10-K\n",
      "<TEXT>\n",
      "<XBRL>\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<!--XBRL Document Created with Wdesk from Workiva-->\n",
      "<!--p:a3804257e71d4ad0beec39065df445b4,x:5b52a4fbf42a489eba7e960790f9d0ea-->\n",
      "<!-- Document created using Wdesk  -->\n",
      "<!-- Copyright 2020 Workiva -->\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:schedoi-fednote=\"http://fasb.org/dis/schedoi-fednote/2019-01-31\" xmlns:fs-interest=\"http://fasb.org/dis/fs-interest/2019-01-31\" xmlns:sfp-ibo=\"http://fasb.org/stm/sfp-ibo/2019-01-31\" xmlns:sfp-sbo=\"http://fasb.org/stm/sfp-sbo/2019-01-31\" xmlns:sic-std=\"http://xbrl.sec.gov/sic-std/2011-01-31\" xmlns:stpr-std=\"http://xbrl.sec.gov/stpr-std-std-std/2018-01-31\" xmlns:currency-ent-std=\"http://xbrl.sec.gov/currency-ent-std/2019-01-31\" xmlns:lea=\"http://fasb.org/dis/lea/2019-01-31\" xmlns:ctbl=\"http://fasb.org/dis/ctbl/2019-01-31\" xmlns:guar=\"http://fasb.org/dis/guar/2019-01-31\" xmlns:ru=\"http://fasb.org/dis/ru/2019-01-31\" xm\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[(df['ticker']=='GOOG') & (df['filing_date'] == '2020-02-04'),'filing'][0][:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading GOOG Filings: 100%|██████████| 3/3 [00:00<00:00,  4.06filling/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "raw_fillings_by_ticker = {}\n",
    "for dt, index_url in tqdm(sec_data.items(), desc=f'Downloading {ticker} Filings', unit='filling'):\n",
    "    file_url = index_url.replace('-index.htm', '.txt').replace('.txtl', '.txt')            \n",
    "    raw_fillings_by_ticker[dt] = sec_api.get(file_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SEC-DOCUMENT>0001652044-20-000008.txt : 20200204\n",
      "<SEC-HEADER>0001652044-20-000008.hdr.sgml : 20200204\n",
      "<ACCEPTANCE-DATETIME>20200203210359\n",
      "ACCESSION NUMBER:\t\t0001652044-20-000008\n",
      "CONFORMED SUBMISSION TYPE:\t10-K\n",
      "PUBLIC DOCUMENT COUNT:\t\t112\n",
      "CONFORMED PERIOD OF REPORT:\t20191231\n",
      "FILED AS OF DATE:\t\t20200204\n",
      "DATE AS OF CHANGE:\t\t20200203\n",
      "\n",
      "FILER:\n",
      "\n",
      "\tCOMPANY DATA:\t\n",
      "\t\tCOMPANY CONFORMED NAME:\t\t\tAlphabet Inc.\n",
      "\t\tCENTRAL INDEX KEY:\t\t\t0001652044\n",
      "\t\tSTANDARD INDUSTRIAL CLASSIFICATION:\tSERVICES-COMPUTER PROGRAMMING, DATA PROCESSING, ETC. [7370]\n",
      "\t\tIRS NUMBER:\t\t\t\t611767919\n",
      "\t\tSTATE OF INCORPORATION:\t\t\tDE\n",
      "\t\tFISCAL YEAR END:\t\t\t1231\n",
      "\n",
      "\tFILING VALUES:\n",
      "\t\tFORM TYPE:\t\t10-K\n",
      "\t\tSEC ACT:\t\t1934 Act\n",
      "\t\tSEC FILE NUMBER:\t001-37580\n",
      "\t\tFILM NUMBER:\t\t20570407\n",
      "\n",
      "\tBUSINESS ADDRESS:\t\n",
      "\t\tSTREET 1:\t\t1600 AMPHITHEATRE PARKWAY\n",
      "\t\tCITY:\t\t\tMOUNTAIN VIEW\n",
      "\t\tSTATE:\t\t\tCA\n",
      "\t\tZIP:\t\t\t94043\n",
      "\t\tBUSINESS PHONE:\t\t650-253-0000\n",
      "\n",
      "\tMAIL ADDRESS:\t\n",
      "\t\tSTREET 1:\t\t1600 AMPHITHEATRE PARKWAY\n",
      "\t\tCITY:\t\t\tMOUNTAIN VIEW\n",
      "\t\tSTATE:\t\t\tCA\n",
      "\t\tZIP:\t\t\t94043\n",
      "</SEC-HEADER>\n",
      "<DOCUMENT>\n",
      "<TYPE>10-K\n",
      "<SEQUENCE>1\n",
      "<FILENAME>goog10-k2019.htm\n",
      "<DESCRIPTION>10-K\n",
      "<TEXT>\n",
      "<XBRL>\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<!--XBRL Document Created with Wdesk from Workiva-->\n",
      "<!--p:a3804257e71d4ad0beec39065df445b4,x:5b52a4fbf42a489eba7e960790f9d0ea-->\n",
      "<!-- Document created using Wdesk  -->\n",
      "<!-- Copyright 2020 Workiva -->\n",
      "<html xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:schedoi-fednote=\"http://fasb.org/dis/schedoi-fednote/2019-01-31\" xmlns:fs-interest=\"http://fasb.org/dis/fs-interest/2019-01-31\" xmlns:sfp-ibo=\"http://fasb.org/stm/sfp-ibo/2019-01-31\" xmlns:sfp-sbo=\"http://fasb.org/stm/sfp-sbo/2019-01-31\" xmlns:sic-std=\"http://xbrl.sec.gov/sic-std/2011-01-31\" xmlns:stpr-std=\"http://xbrl.sec.gov/stpr-std-std-std/2018-01-31\" xmlns:currency-ent-std=\"http://xbrl.sec.gov/currency-ent-std/2019-01-31\" xmlns:lea=\"http://fasb.org/dis/lea/2019-01-31\" xmlns:ctbl=\"http://fasb.org/dis/ctbl/2019-01-31\" xmlns:guar=\"http://fasb.org/dis/guar/2019-01-31\" xmlns:ru=\"http://fasb.org/dis/ru/2019-01-31\" xm\n"
     ]
    }
   ],
   "source": [
    "print(raw_fillings_by_ticker['2020-02-04'][:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"2020-02-04.html\",'w') as fh:\n",
    "    fh.write(raw_fillings_by_ticker['2020-02-04'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the 10-K filing from the 10-K form\n",
    "\n",
    "This information is extracted by looking for pattern  \n",
    "`\n",
    "<DOCUMENT>\n",
    "<TYPE>10-K\n",
    "...\n",
    "...\n",
    "</DOCUMENT>\n",
    "`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"\"\"<DOCUMENT>\n",
    "<TYPE>10-K\n",
    "<p>line 1</p>\n",
    "<br>line 2\n",
    "</DOCUMENT>\n",
    "<DOCUMENT>\n",
    "line 3\n",
    "line 4\n",
    "</DOCUMENT>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n<TYPE>10-K\\n<p>line 1</p>\\n<br>line 2\\n', '\\nline 3\\nline 4\\n']\n",
      "match:  <re.Match object; span=(1, 37), match='<TYPE>10-K\\n<p>line 1</p>\\n<br>line 2\\n'>\n",
      "<p>line 1</p>\n",
      "<br>line 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def get_10K_filing(text):\n",
    "    \"\"\"\n",
    "    Extract the documents from the text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text with the document strings inside\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    extracted_docs : list of str\n",
    "        The document strings found in `text`\n",
    "    \"\"\"\n",
    "\n",
    "    regex_start = re.compile(r'<DOCUMENT>')\n",
    "    matches = regex_start.finditer(text)\n",
    "    start_pos =[match.span()[1] for match in matches]\n",
    "\n",
    "    regex_end = re.compile(r'</DOCUMENT>')\n",
    "    matches_end = regex_end.finditer(text)\n",
    "    end_pos = [match.span()[0] for match in matches_end]\n",
    "    \n",
    "    #print('start: ', start_pos, 'end: ', end_pos)\n",
    "    extracted_docs = [text[start:end] for start, end in zip(start_pos,end_pos)]\n",
    "    print(extracted_docs)\n",
    "    for doc in extracted_docs:\n",
    "        regex = re.compile(r'<TYPE>10-K\\s*\\n(.*)', re.DOTALL)\n",
    "        matches = regex.finditer(doc)\n",
    "        for match in matches:\n",
    "            print('match: ', match)\n",
    "            \n",
    "    return match.group(1)\n",
    "\n",
    "print(get_10K_filing(test_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3446213"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_10K_filing(raw_fillings_by_ticker['2020-02-04']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"10k-2020-02-04.html\",'w') as fh:\n",
    "    fh.write(get_10K_filing(raw_fillings_by_ticker['2020-02-04']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Document</title>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs4_obj = BeautifulSoup(raw_fillings_by_ticker['2020-02-04'])\n",
    "bs4_obj.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2\n",
      "1 2\n",
      "2 2\n",
      "3 2\n",
      "4 2\n",
      "5 2\n",
      "6 2\n",
      "7 2\n",
      "8 2\n",
      "9 2\n",
      "10 2\n",
      "11 2\n",
      "12 2\n",
      "13 2\n",
      "14 2\n"
     ]
    }
   ],
   "source": [
    "for i, docs in enumerate(bs4_obj.find_all('document')):\n",
    "    for result in docs.find_all('type'):\n",
    "        print(i, len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 354067 None >>> 10-K\n",
      "1\n",
      "goo\n",
      "1 32786 None >>> EX-4.14\n",
      "2\n",
      "\n",
      "2 19335 None >>> EX-10.08.1\n",
      "3 31587 None >>> EX-10.08.2\n",
      "4 520 None >>> EX-21.01\n",
      "5\n",
      "5 1686 None >>> EX-23.01\n",
      "6\n",
      "6 3475 None >>> EX-31.01\n",
      "7\n",
      "7 3491 None >>> EX-31.02\n",
      "8\n",
      "8 1971 None >>> EX-32.01\n",
      "9\n",
      "9 13555 None >>> EX-101.SCH\n",
      "10 871 None >>> EX-101.CAL\n",
      "11 1679 None >>> EX-101.DEF\n",
      "12 84691 None >>> EX-101.LAB\n",
      "13 2478 None >>> EX-101.PRE\n",
      "14 10336390 None >>> GRAPHIC\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "for i, docs in enumerate(bs4_obj.find_all('type')):\n",
    "        print(i, len(docs.get_text()), docs.p, \">>>\", docs.get_text()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 found 10-k\n",
      "1 found 10-k\n",
      "2 not found\n",
      "3 not found\n",
      "4 not found\n",
      "5 found 10-k\n",
      "6 found 10-k\n",
      "7 found 10-k\n",
      "8 found 10-k\n",
      "9 not found\n",
      "10 not found\n",
      "11 not found\n",
      "12 not found\n",
      "13 not found\n",
      "14 found 10-k\n"
     ]
    }
   ],
   "source": [
    "for i, docs in enumerate(bs4_obj.find_all('type')):\n",
    "    if docs.find(text=re.compile('10-K', flags=re.DOTALL)):\n",
    "        print(i, \"found 10-k\")\n",
    "    else:\n",
    "        print(i, \"not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return val: \n",
      " line 1\n",
      "line 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_10K_filing2(text):\n",
    "    \"\"\"\n",
    "    Extract the documents from the text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The text with the document strings inside\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    extracted_docs : list of str\n",
    "        The document strings found in `text`\n",
    "    \"\"\"\n",
    "    extracted_docs = BeautifulSoup(text).find_all('document')\n",
    "    plain_text=\"\"\n",
    "    for i, doc in enumerate(extracted_docs):\n",
    "        \n",
    "        #print('processing ', i, \">>\\n\",str(doc))\n",
    "        regex = re.compile(r'.*<type>10-K\\s*\\n(.*)</type>', re.DOTALL)\n",
    "        matches = regex.finditer(str(doc))\n",
    "        for match in matches:\n",
    "            # print('match: ', match)\n",
    "            plain_text = BeautifulSoup(match.group(1), 'html.parser').get_text() # remove all the html\n",
    "            return plain_text\n",
    "    return plain_text\n",
    "print('return val: \\n', get_10K_filing2(test_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"10k-cleaned-2020-02-04.html\",'w') as fh:\n",
    "    fh.write(get_10K_filing2(raw_fillings_by_ticker['2020-02-04']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match:  <re.Match object; span=(0, 3417472), match='<document>\\n<type>10-K\\n<sequence>1\\n<filename>go>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3417443"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_10K_filing2(raw_fillings_by_ticker['2020-02-04']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3446213"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_10K_filing(raw_fillings_by_ticker['2020-02-04']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28770"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3446213-3417443"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
